[TOC]

## DNN 深层神经网络 

### 深度学习的方法

#### 1.大致流程图

![1531969558078](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_深度学习方法流程图)

不要 看到所有不好的结果就说是overfitting（过拟合）   

#### 2.Training Data 不同误差采取措施

![1531980561034](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_解决方法)

##### Learning Rate的改进

在深层神经网络中，我们会发现一种情况，就是在初始层的梯度下降比较慢，而随着层次增高，梯度下降收敛的速度逐渐增快。而为什么会发生这种情况呢？

![1531981352891](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_神经网络参数改变)

我们对于某一个参数对C的偏微分，如果对该参数进行了变动之后，将会产生什么样的影响呢？我们把第一个layer中的某一个参数 +△w，那么我们可以观看一下变化。

假设我们的△w值很大，通过了其中的sigmoid函数之后，将会变小。

![1531981851944](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_sigmoid函数)

我们可以看到即使△w很大，但是在通过了sigmoid函数之后，它的值也会逐渐变小。而随着层数的增加，到了最后，无论前面的△w值多大，在最后的输出所产生的影响都很小，并且不突出。因此我们可以发现或许sigmoid函数并不适合作为神经元的activation function。

**那应该怎么改进呢**？

###### ReLU线性整流函数 

又称**修正线性单元**, 是一种人工神经网络中常用的激活函数 。

![1532266341995](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_ReLU )

它的数学表达式是：
$$
 f(x)=max(0,x)
$$
特点比较：

|                          优点                           |                           缺点                           |
| :-----------------------------------------------------: | :------------------------------------------------------: |
| 使用 ReLU 得到的SGD  的收敛速度会比 sigmoid/tanh 快很多 | ReLU在训练的时候很”脆弱”，一不小心有可能导致神经元”坏死” |

举个例子：由于ReLU在x<0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是0了，也就是ReLU神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都坏死了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。 

###### Leaky ReLUs

Leaky ReLUs 就是用来解决ReLU坏死的问题的。和ReLU不同，当x<0时，它的值不再是0，而是一个较小斜率(如0.01等)的函数。也就是说f(x)=1(x<0)(ax)+1(x>=0)(x),其中a是一个很小的常数。这样，既修正了数据分布，又保留了一些负轴的值，使得负轴信息不会全部丢失。 

![1532266846665](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_Leaky ReLu )



###### Maxout

![1533011734831](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_maxout)

在maxout中假设输入值为x1与x2，在隐藏层中进行4种权重计算，得到（5,7，-1,1）这四个输出，而这个输出中随机分成两组（也可以分成多组）从这两组中选择最大的输出结果作为下一层的输入值。依次类推，这种神经网络的算法过程即为maxout。

* ReLU 也是maxout中的一种特殊情况。但maxout不仅仅是ReLU。

![1533014582122](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_maxout1)

![1533014879005](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_maxout3)

因此我们可以知道maxout是一种**可学习的刺激函数**。 

* 函数包含多少段，取决于有多少元素存放于一个组中。

![1533015170582](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_maxout4)

##### 最佳网络参数

当我们梯度下降的时候，存在梯度下降卡在鞍点，卡在平滑处，以及卡在局部最小值区域时都可能存在偏导数为0.

![1532266846665](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_偏导为0的点  )

而在实际中我们往往会用**RMSProp+Monmentum**来进行计算，这样能够尽量避免出现上述情况。

![1532835501105](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_Momentum)

###### Adam

Adam 是一种可以替代传统随机梯度下降过程的一阶优化算法，它能基于训练数据迭代地更新神经网络权重。Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。 Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。 

#### 3.Testing Data 不同误差采取措施

![1532835822500](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_解决方法1)

##### Early Stopping 早停止

对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。    

Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30…… 

早停止实际上是一种学习时的策略。通常我们在训练一个模型的时候，如果模型过于强大，或者训练数据过少等原因，很容易会导致过拟合等问题的产生。不幸的是，这种情况几乎是必然发生的。因为随着训练的深入，你的模型会越来越倾向于学习到“训练数据”的规律，而不是“生成训练数据的模型”的规律。这个时候，就是我们常说到的“过拟合”了。 

发生过拟合的一个现象，就是训练误差足够小，但是测试误差却比较大。例如下面这张图 

![1533009605780](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_early stop)

先忽略掉bias和variance这两件事，我们只看坐标轴和这两条曲线。

这张图的纵轴是指误差，我们的目标就是让这个误差最小化。然而对于模型来说，有两个误差：一是训练过程中的训练误差（青线），二是显示模型真正能力的测试误差（红线）。

对于横轴，我们可以从两个角度来考虑。

第一个角度是模型的复杂性。也就是说，一般我们的模型越复杂，就越容易训练出来一个“过拟合”的模型。

第二个角度是训练的“深入程度”，也就是“训练周期”的数量。当模型训练周期增加，就很容易出现“训练误差越来越小，测试误差却越来越大”的情况。这时候，模型就是过拟合了。

![1533009771603](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_early stop2)

这张图是一个比较经典的训练过程的描述，我们发现随着Time的增加，训练集的损失不断在减小，但测试集(此处是Validation集)的误差却是先减小，后缓慢增加。

这时候，早停止的策略呼之欲出：在测试误差开始上升之前，就将训练停下来。因为在这个“提前停下来”的位置，尽管训练误差并没有达到最小，但是如果继续训练，测试误差就会变得更大，得不偿失了。

然而这时候要考虑一个小问题，就是测试误差的曲线是震荡的，不能因为一次微小的上升就认为整个训练应该结束，因为继续训练有可能获得更好的结果。

考虑清楚了这个问题，这个早停止的算法也就简单粗暴地呈现出来了：

首先我们要保存好现在的模型（网络结构和权值）。训练n次，然后用validation集测试一次，如果发现测试误差与上一次做evaluation的误差相比有下降，就再将现在的模型保存下来。如果发现比上次evaluation的性能差，那也不立刻终止，而是往后再看几步。直到经过了p次evaluation，如果测试误差依旧没有下降，那就认为这个实验应该早在上一次达到最低测试误差的时候停下来。

那么，为什么这个算法能起到“正则化”的效果？也就是为什么它能防止overfit？这就要从定性和定量两个角度来考虑了。

定性来说，我们可以从这张图中看出一些端倪：

![1533009913421](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_early stop3)

![1533009969632](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_early stop4)

![1533009999345](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_early stop5)

##### 正则化(L0、L1与L2范数)

![1533022876661](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化)

![1533023003265](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化2)

![1533023070317](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化3)

![1533023100057](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化4)

![1533023137374](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化5)

![1533023193563](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化6)

![1533023226083](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化7)

![1533023257325](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化8)

![1533023303227](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化9)

![1533023346696](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化10)

![1533023417853](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_正则化11)

##### Droupout

![1533008519266](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_droupout)

上图为Dropout的可视化表示，左边是应用Dropout之前的网络，右边是应用了Dropout的同一个网络。

Dropout的思想是训练整体DNN，并平均整个集合的结果，而不是训练单个DNN。DNNs是以概率P舍弃部分神经元，其它神经元以概率q=1-p被保留，舍去的神经元的输出都被设置为零。

在标准神经网络中，每个参数的导数告诉其应该如何改变，以致损失函数最后被减少。因此神经元元可以通过这种方式修正其他单元的错误。但这可能导致复杂的协调，反过来导致过拟合，因为这些协调没有推广到未知数据。Dropout通过使其他隐藏单元存在不可靠性来防止共拟合。

简而言之：Dropout在实践中能很好工作是因为其在训练阶段阻止神经元的共适应。

###### Droupout如何工作

![1533023572678](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_droupout1)

![1533023599077](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_droupout2)

![1533023657013](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_droupout3)

![1533023734908](D:\面试md\机器学习李宏毅\18 Tips for Training DNN\18_droupout4)